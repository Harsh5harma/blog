[
  {
    "objectID": "posts/stable-diffusion/index.html",
    "href": "posts/stable-diffusion/index.html",
    "title": "Stable Diffusion for dummies (written by a dummy)",
    "section": "",
    "text": "Most people reading this have probably heard of image generation at some point in time. If you’re among those who haven’t, then this journey will be even better as I go in some fair detail to try and explain what it is.\nIf at any point, the things I talk about seem hard, don’t be discouraged as it’s more of a shock if you find this easy to read. Nevertheless, buckle up because although we might not go all the way into the deep-end (I don’t know enough stuff yet), I think we’ll still get to dip our toes.\n\n\n\nI’ll just let the images speak.\n\n\n\n\n\n\n\n\ncozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful\n\n\n\n\n\n\n\nDune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.\n\n\n\n\n\n\n\n\n\ninspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space\n\n\n\n\n\n\n\nAll of the above were generated with simple but detailed text prompts.\n\n\n\n\n\n\nCaution\n\n\n\nThis blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you’re a bit rusty with some of the terms, I’ve defined most used in this blog.\n\n\n\n\n\n\n\n\nJargon\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nparameters\nthe values in the model that change what task it can do, updated through model training\n\n\narchitecture\nThe mathematical function that we pass input data and parameters to\n\n\nmodel\nThe combination of the architecture with a particular set of parameters\n\n\ngradient\nA Tensor that tells us how the vector field changes in any direction\n\n\nloss\nA measure of how good the model is, chosen to drive training via stochastic gradient descent\n\n\nmetric\nA human readable way to measure the performance of a model\n\n\nCNN\nConvolutional Neural Network; a type of NN that works well for computer vision tasks\n\n\nembeddings\nA relatively-low dimension space into which we can translate high-dimensional vectors\n\n\nlatent\nA latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image\n\n\nlatent space\nA feature or embedding space where similar embeddings lie close to each other, and different farther.\n\n\nnoise\nRandom variation of brightness or color information in images\n\n\nU-Net\n\n\n\nVAE\nVariational Auto Encoder\n\n\nCLIP\nContrastive Language-Image Pre-Training, a visual classification NN\n\n\n\n\n\n\nIf you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you’re like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment.  \nImagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit.  \nLet the probability be \\(P(X)\\) for an input Image X.\n\n\n\n\nflowchart LR\n  I1(X1) --&gt; F((Magic Function))\n  I2(X2) --&gt; F\n  I3(X3) --&gt; F\n  F --&gt; O1(\"P(X1)\")\n  F --&gt; O2(\"P(X2)\")\n  F --&gt; O3(\"P(X3)\")\n\n\n\n\n\n\n\n\nWait. How do we use this function to generate new digits?\nWe do the following steps:\n\n\nTry making a pixel of X darker and then check the output probability\nTry making a pixel of X lighter and then check probability\nDo this for every pixel in X (If X is a 28x28 image, that’s 784 times!)\n\nIn the above steps, we’re calculating the change in probability of the images being handwritten wrt. to each pixel in the image (784 for a 28x28 image), mathematically it can be represented as follows: \\[\\nabla_X P(X)\\]\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\nabla_X P(X)\\) is itself a vector of size 784.\n\n\nNext step we do is to multiply \\(\\nabla_X P(X)\\) with some constant. \\[c*\\nabla_X P(X)\\]\nNow we subtract the above term from the 784 pixel values of \\(X\\) and in turn increase the output probability, and we do it a bunch of times.\n\n\n\n\n\nflowchart LR\n  I(X) --&gt; F((Magic Function))\n  F --&gt; O(\"P(X)\")\n  O --&gt; G[\"c * ∇ₓP(X)\"]\n  G --&gt; S[\"-\"]\n  S --&gt; I\n\n\n\n\n\n\n\nHaving gone through all that, it would be an intelligent guess to say that now all we need is for someone to give us the magic function so we can start generating.\n\nBut there’s some bad news, nobody is going to hand us any magical function that takes care of everything under the hood. We’re going to have to train our own neural network on a lot of handwritten data to do this for us.\n\nSeems simple enough to do for any seasoned DL practitioner. Alas! There’s a catch. Now I don’t know the why or how, but smarter people than me figured out that it’s pretty much practically impossible if not insanely hard to create a metric that tells us how much our image looks like a specific digit.\n\n\n\n\n\n\nImportant\n\n\n\nDon’t forget that our end goal is making super pretty images like we saw at the start, not some basic digits.\n\n\n\n\nSo, they decided on an interesting but very logical approach.\n\nEvery image we train on can be overlayed with some random amounts of noise, the result would be a noisier image of course but now we have some place to start with.\nNoisy Image = Noise + Clear Image\nWkt we can’t predict how much an image looks like something, but turns out we can try and predict how noisy an image is, and then using basic arithmetic we can see that we’d just have to subtract that predicted noise from the noisy image to end up with a relatively Clear Image.\n\nPassing in a starting pic and repeating this over and over would train our model to get good at predicting the noise in an image and getting rid of it.\n\nThis model is very similar to a U-Net\n\n\n\nProbability of Noise:\n\n\n\n\n\nflowchart LR\n    B[\"Randomly selected sequential noise\"] --&gt;  X((+))\n    X --&gt; A[\"Somewhat noisy latent\"] --&gt; C((\"Diffusion Model\"))\n    C --&gt; D(\"Predicted Noise\") --&gt; - --&gt; A\n    A --&gt; E[\"Less noisy latent\"]\n    F[\"Randomly Selected Noise\"] --&gt; Z((+)) --&gt; E --&gt;\n    G((\"Updated Diffusion Model\")) --&gt; H(\"Predicted Noise\")\n    H --&gt; Y[-] --&gt; E\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nNotes\n\n\n\n\nUnet\nsomewhat noisy latents\nthe noise\n-\n\n\nVAE Decoder\nsmall latents tensor\nlarge image\nguided by captions\n\n\nCLIP text encoder\ntext\nembedding\n-"
  },
  {
    "objectID": "posts/stable-diffusion/index.html#cool-pics-to-pump-you-up",
    "href": "posts/stable-diffusion/index.html#cool-pics-to-pump-you-up",
    "title": "Stable Diffusion for dummies (written by a dummy)",
    "section": "",
    "text": "I’ll just let the images speak.\n\n\n\n\n\n\n\n\ncozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful\n\n\n\n\n\n\n\nDune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.\n\n\n\n\n\n\n\n\n\ninspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space\n\n\n\n\n\n\n\nAll of the above were generated with simple but detailed text prompts.\n\n\n\n\n\n\nCaution\n\n\n\nThis blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you’re a bit rusty with some of the terms, I’ve defined most used in this blog.\n\n\n\n\n\n\n\n\nJargon\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nparameters\nthe values in the model that change what task it can do, updated through model training\n\n\narchitecture\nThe mathematical function that we pass input data and parameters to\n\n\nmodel\nThe combination of the architecture with a particular set of parameters\n\n\ngradient\nA Tensor that tells us how the vector field changes in any direction\n\n\nloss\nA measure of how good the model is, chosen to drive training via stochastic gradient descent\n\n\nmetric\nA human readable way to measure the performance of a model\n\n\nCNN\nConvolutional Neural Network; a type of NN that works well for computer vision tasks\n\n\nembeddings\nA relatively-low dimension space into which we can translate high-dimensional vectors\n\n\nlatent\nA latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image\n\n\nlatent space\nA feature or embedding space where similar embeddings lie close to each other, and different farther.\n\n\nnoise\nRandom variation of brightness or color information in images\n\n\nU-Net\n\n\n\nVAE\nVariational Auto Encoder\n\n\nCLIP\nContrastive Language-Image Pre-Training, a visual classification NN\n\n\n\n\n\n\nIf you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you’re like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment.  \nImagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit.  \nLet the probability be \\(P(X)\\) for an input Image X.\n\n\n\n\nflowchart LR\n  I1(X1) --&gt; F((Magic Function))\n  I2(X2) --&gt; F\n  I3(X3) --&gt; F\n  F --&gt; O1(\"P(X1)\")\n  F --&gt; O2(\"P(X2)\")\n  F --&gt; O3(\"P(X3)\")\n\n\n\n\n\n\n\n\nWait. How do we use this function to generate new digits?\nWe do the following steps:\n\n\nTry making a pixel of X darker and then check the output probability\nTry making a pixel of X lighter and then check probability\nDo this for every pixel in X (If X is a 28x28 image, that’s 784 times!)\n\nIn the above steps, we’re calculating the change in probability of the images being handwritten wrt. to each pixel in the image (784 for a 28x28 image), mathematically it can be represented as follows: \\[\\nabla_X P(X)\\]\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\nabla_X P(X)\\) is itself a vector of size 784.\n\n\nNext step we do is to multiply \\(\\nabla_X P(X)\\) with some constant. \\[c*\\nabla_X P(X)\\]\nNow we subtract the above term from the 784 pixel values of \\(X\\) and in turn increase the output probability, and we do it a bunch of times.\n\n\n\n\n\nflowchart LR\n  I(X) --&gt; F((Magic Function))\n  F --&gt; O(\"P(X)\")\n  O --&gt; G[\"c * ∇ₓP(X)\"]\n  G --&gt; S[\"-\"]\n  S --&gt; I\n\n\n\n\n\n\n\nHaving gone through all that, it would be an intelligent guess to say that now all we need is for someone to give us the magic function so we can start generating.\n\nBut there’s some bad news, nobody is going to hand us any magical function that takes care of everything under the hood. We’re going to have to train our own neural network on a lot of handwritten data to do this for us.\n\nSeems simple enough to do for any seasoned DL practitioner. Alas! There’s a catch. Now I don’t know the why or how, but smarter people than me figured out that it’s pretty much practically impossible if not insanely hard to create a metric that tells us how much our image looks like a specific digit.\n\n\n\n\n\n\nImportant\n\n\n\nDon’t forget that our end goal is making super pretty images like we saw at the start, not some basic digits.\n\n\n\n\nSo, they decided on an interesting but very logical approach.\n\nEvery image we train on can be overlayed with some random amounts of noise, the result would be a noisier image of course but now we have some place to start with.\nNoisy Image = Noise + Clear Image\nWkt we can’t predict how much an image looks like something, but turns out we can try and predict how noisy an image is, and then using basic arithmetic we can see that we’d just have to subtract that predicted noise from the noisy image to end up with a relatively Clear Image.\n\nPassing in a starting pic and repeating this over and over would train our model to get good at predicting the noise in an image and getting rid of it.\n\nThis model is very similar to a U-Net"
  },
  {
    "objectID": "posts/stable-diffusion/index.html#specifics-of-training",
    "href": "posts/stable-diffusion/index.html#specifics-of-training",
    "title": "Stable Diffusion for dummies (written by a dummy)",
    "section": "",
    "text": "Probability of Noise:\n\n\n\n\n\nflowchart LR\n    B[\"Randomly selected sequential noise\"] --&gt;  X((+))\n    X --&gt; A[\"Somewhat noisy latent\"] --&gt; C((\"Diffusion Model\"))\n    C --&gt; D(\"Predicted Noise\") --&gt; - --&gt; A\n    A --&gt; E[\"Less noisy latent\"]\n    F[\"Randomly Selected Noise\"] --&gt; Z((+)) --&gt; E --&gt;\n    G((\"Updated Diffusion Model\")) --&gt; H(\"Predicted Noise\")\n    H --&gt; Y[-] --&gt; E\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nNotes\n\n\n\n\nUnet\nsomewhat noisy latents\nthe noise\n-\n\n\nVAE Decoder\nsmall latents tensor\nlarge image\nguided by captions\n\n\nCLIP text encoder\ntext\nembedding\n-"
  },
  {
    "objectID": "calculation.html",
    "href": "calculation.html",
    "title": "Theory Threads and Dev Doodles",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for time steps and corresponding noise levels\ntime_steps = np.arange(0, 1000, 1)  # Assuming 100 time steps\nnoise_levels =[-(x)**2 + 30*x + 2 for x in range(1,1001)] # Random noise levels between 0 and 1\n\n# Plotting the time-step noise schedule graph\nplt.figure(figsize=(10, 6))\nplt.plot(time_steps, noise_levels, color='b', label='Noise Levels')\nplt.xlabel('Time Steps')\nplt.ylabel('Noise Level')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Harsh Sharma, a 20 yr old CS student with a new found interest for Deep learning. I love writing code and learning concepts that make me tear my hair out ; )\nI was introduced to the basics of HTML, CSS and some python in middle school and early high school, but I started programming much more seriously a few months before starting college.\nI also share a deep love for core CS concepts and the most recent project I did was related to writing my own little toy language. The languages I’m comfortable with include Python, C++, C, Javascript (and maybe Java). Other than that I’m also quite familar with HTML&CSS and am currently learning React as well.\nTech stuff apart, I love reading and listening to music. Current book of mine is The Republic by Plato and my favorite band atm is Nine Inch Nails (Ghosts I-IV 🤘)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theory Threads and Dev Doodles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nStable Diffusion for dummies (written by a dummy)\n\n\n\n\n\n\n\nNotes\n\n\nFastai Part2\n\n\n\n\nA little informal guide for diffusion first timers.\n\n\n\n\n\n\nOct 29, 2023\n\n\nHarsh Sharma\n\n\n\n\n\n\nNo matching items"
  }
]