[
  {
    "objectID": "posts/stable-diffusion/index.html",
    "href": "posts/stable-diffusion/index.html",
    "title": "Noisy to Nice",
    "section": "",
    "text": "About this blog\n\n\n\n\n\nThis is my first ever blog and it’s on a topic I find relatively hard, although I’ve tried my best to be accurate in whatever I write, It won’t be surprising if this entire blog was riddled with inaccuracies and straight up incorrect stuff. I simply don’t have that necessary but aquired oversight yet. So I’m asking the reader to help me out with any errors they might spot and I’ll happily correct whatever it is that I presented wrong."
  },
  {
    "objectID": "posts/stable-diffusion/index.html#vae-variational-auto-encoder",
    "href": "posts/stable-diffusion/index.html#vae-variational-auto-encoder",
    "title": "Noisy to Nice",
    "section": "4.1 VAE: Variational Auto Encoder",
    "text": "4.1 VAE: Variational Auto Encoder\n\n\n\nVariational Auto Encoder\n\n\n\nIn the context of our model, VAE is just a data compression and decompression algorithm with a fancy name. \nVAE tries that what came in as input is the same as what comes out, it’s architecture might look similar to U-Net but VAE does not have cross connections unlike the former and VAE is also an autoencoder. \nThe special thing about autoencoders is that they can split in half and we can just use the encoder and decoder seperately.\nThe encoder serves as a great compression method and on the \\(512*512*3\\) example it reduces the pixel count from \\(786,432\\) to a mere \\(16,384\\). A size reduction of \\(48\\) times.\n\n\n\n\n\n\nImportant\n\n\n\nThe encoder peforms compression by doing stride 2 convolutions and doubling the channel size, this helps in increasing the complexity and expressive power of the model without increasing the spatial feature maps.\nSo, a stride 2 convolution reduces the spatial dimensions and memory usage and additional channels give the model more capacity to learn.\n\n\nAnd if we have a copy of the decoder, we can feed the encoded latent to it and end up with the high resolution image again.\nThis model serves a great compression algorithm because it’s been trained on millions upon millions of input images, at the start of its training process, the VAE spits out random noise out of it’s decoder but if we use the humble Mean Squared Error function on the encoder input and decoder output to train the VAE reduce the loss.\nHow it works is interesting though. VAE, is built up of two main components and each of them can work independently once the autoencoder has been trained.\n\nThe first part consists of the Encoder. This is where compression happens, we start with a high resolution image of \\(512*512*3\\) and keep performing convolutions of stride 2 and doubling the channels till we end up with a relatively much lower dimension image. This image is called the latent and the multi-dimensional space it exists in is called the latent space.\n\n\n\n\n\n\nNote\n\n\n\nA latent contains the most interesting and useful information of an image.\nA channel in a CNN refers to a specific component of the image data, in the case of a colour image, the channels typically represent different colors.\nA feature in CNN refers to specific characteristics or pattern in the input data that the network learns to recognise. These patterns can be simple or complex, early layers of a CNN features are basic patterns and deeper in the network, they’re more abstract and complex.\n\n\n\n\n\n\n\n\nNote\n\n\n\nA stride two convolution means that at each step, the convolution skips a column and a row. So for a \\(4*4\\) grid, a stride 2 convolution would bring the size down\n\n\n\n\n\n\nflowchart LR\n  A[\"High Res Image\"] --&gt; M((\"VAE Encoder\"))\n  M --&gt; O[\"Image Latent Tensor\"]\n\n\n\n\n\nNow let’s get back to what we need VAE for, the size problems our older model had. Since we can compress image sizes by godly amounts, there’s no doubt, that we’ll train the U-Net model with these encoded images (latents).\nSo, if our U-Net needs to be trained on 10 million images, we’ll pass each one of them through VAE's encoder first, and then we feed these latents into the U-Net 100s to 1000s of times.\n\n\n\n\n\n\nCaution\n\n\n\nDon’t forget that U-Net denoises input images through repetition.\n\n\nNow our model looks like this:\n\n\n\n\nflowchart LR\n  I[\"Noisy Latent Tensor\"] == \"(1)\" ==&gt; M((\"U-Net\"))\n  G[\"OHE Guidance Vector\"] == \"(1, 6)\" ==&gt; M\n  M == \"(2)\" ==&gt; O[\"Predicted Noise\"]\n  O == \"(3)\" ==&gt; Z[\"Subtracted From\"]\n  Z == \"(4)\" ==&gt; I\n  I == \"(5)\" ==&gt; nI[\"Less Noisy Latent Tensor\"]\n  nI == \"(6)\" ==&gt; M\n\n\n\n\n\nWouldn’t this new latent based model spit out latents too? Yes, it will but we don’t forget that we also have the other half of our autoencoder. The VAE decoder will be able to take in the generated latent as input and then output a high resolution image. It’d be like there wasn’t any compression-decompression to begin with.\n\n\n\n\nflowchart LR\n  A[\"U-Net Output Latent\"] --&gt; M((\"VAE Decoder\"))\n  M --&gt; O[\"High Res Version of Generated Image\"]\n\n\n\n\n\nTechnically we don’t need to use an autoencoder, but it greatly saves on computation necessary and the time it takes to generate an image as well as the modern training itself.\n\n\n\n\n\n\nNote\n\n\n\nVAE encoder: Used during model training.\nVAE decoder: Used during model inference."
  },
  {
    "objectID": "posts/stable-diffusion/index.html#clip",
    "href": "posts/stable-diffusion/index.html#clip",
    "title": "Noisy to Nice",
    "section": "4.2 CLIP",
    "text": "4.2 CLIP\nIgnore the weird name for now and focus on the second problem we had. Which was, finding a way to create embeddings for any possible text and not just the digits b/w 0 to 9 so we could guide the model.\nWhat if we had a model that spat out a vector representation of every complex sentence, such that we could pass it with our latent tensor as guidance. And what if, that model created textual embeddings in such a way, that they ended up being close to the latent tensor’s embedding in the latent space.\n\n\n\n\n\n\nMy personal intuition for this, could be wrong.\n\n\n\n\n\nIf the above paragraph was hard to get, I can explain with a more grounded example. I’m aware that the human brain and neural networks are not at all the same in the manner with which they function, and are designed but I couldn’t think of an easier example, so bear with me.\nImagine a guitar with a few broken strings, after reading this text could your brain think up of an image showing the same? It might not be super clear, 4k quality or something, but you could in your head think of an image that had the most important features that a guitar with a few broken strings might have.\nI think that a latent space is just that neural network analog for that headspace we go into. Our brain was able to process and break-down the text and then also match it to a visual representation.\nSo we essentially need a model, that is capable of doing something similar to this in essence. CLIP is one such model that does this job.\n\n\n\nCLIP is made up of two models. A Text-encoder and an Image-encoder.\n\n\n\n\nflowchart TD\n  T[\"Input text\"] --&gt; TE([\"CLIP Text Encoder\"])\n  I[\"Input Image\"] --&gt; IE([\"CLIP Image Encoder\"])\n  TE --&gt; VT[\"Vector Representation of Text\"]\n  IE --&gt; VI[\"Vector Representation of Image\"]\n  VT --&gt; LS((\"Latent Space\"))\n  VI --&gt; LS\n\n\n\n\n\nBoth encoders need to be trained first with randomly selected weights. The ‘CL’ in CLIP stands for contrastive loss and that’s what helps in training both the text and image encoders. Fancy name apart, training process is quite simple.\nLet’s take three random text prompts:\nText 1: Muscular Hen’s caricature\nText 2: Broken Egg\nText 3: Senior man with long hair and beard portrait\nWe’ll call the vector representation of each text prompt as T1, T2, T3 respectively.\nWe also now have 3 images of the same, and we’ll use I1, I2, I3 to represent the vector representations of these images. All of these vector representations were generated by throwing them into the CLIP encoders (and they’re random right now because we haven’t trained CLIP yet).\nNow, we can make a grid with the Image vectors lined up along the columns and the Text vectors lined along the rows.\n\n\n\nTraining Grid\n\n\nEach cell of the grid contains the dot product of the two representation vectors. The dot product of vectors that actually describe an image with the right text will be the highest, i.e. of the diagonal cells.\nThe non-diagonal cells on the other hand, should have low values once the model is trained, because the embeddings are far apart from each other in the latent-space. The text representation vector is not correct for the image.\nNow as the final step, we sum up all the diagonals together and we also sum up all the non-diagonals separately.\n\n\\(Sum_d = \\text{Sum of Diagonals}\\)\n\\(Sum_nd = \\text{Sum of Non-Diagonals}\\)\n\\(Loss = Sum_d - Sum_nd\\)\n\nThe loss function is known as Contrastive Loss, and it should be zero if all our generated embeddings are perfect matches.\nSo, to conclude this section. CLIP is a multi-modal model as our embeddings all share the same space and it helps us solve our second problem of not having a way to create embeddings for complex text."
  },
  {
    "objectID": "calculation.html",
    "href": "calculation.html",
    "title": "Theory Threads and Dev Doodles",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for time steps and corresponding noise levels\ntime_steps = np.arange(0, 1000, 1)  # Assuming 100 time steps\nnoise_levels =[-(x)**2 + 30*x + 2 for x in range(1,1001)] # Random noise levels between 0 and 1\n\n# Plotting the time-step noise schedule graph\nplt.figure(figsize=(10, 6))\nplt.plot(time_steps, noise_levels, color='b', label='Noise Schedule')\nplt.xlabel('Time Steps')\nplt.ylabel('Noise Schedule')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Harsh Sharma, a 20 yr old CS student with a new found interest for Deep learning. I love writing code and learning concepts that make me tear my hair out ; )\nI was introduced to the basics of HTML, CSS and some python in middle school and early high school, but I started programming much more seriously a few months before starting college.\nI also share a deep love for core CS concepts and the most recent project I did was related to writing my own little toy language. The languages I’m comfortable with include Python, C++, C, Javascript (and maybe Java). Other than that I’m also quite familar with HTML&CSS and am currently learning React as well.\nTech stuff apart, I love reading and listening to music. Current book of mine is The Republic by Plato and my favorite band atm is Nine Inch Nails (Ghosts I-IV 🤘)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theory Threads and Dev Doodles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nNoisy to Nice\n\n\n\n\n\n\n\nNotes\n\n\nFastai Part2\n\n\n\n\nAn amateur’s guide to stable diffusion.\n\n\n\n\n\n\nOct 31, 2023\n\n\nHarsh Sharma\n\n\n\n\n\n\nNo matching items"
  }
]