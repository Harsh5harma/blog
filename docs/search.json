[
  {
    "objectID": "posts/stable-diffusion/index.html",
    "href": "posts/stable-diffusion/index.html",
    "title": "Stable Diffusion for dummies (written by a dummy)",
    "section": "",
    "text": "Most people reading this have probably heard of image generation at some point in time. If you’re among those who haven’t, then this journey will be even better as I go in some fair detail to try and explain what it is.\nIf at any point, the things I talk about seem hard, don’t be discouraged as it’s more of a shock if you find this easy to read. Nevertheless, buckle up because although we might not go all the way into the deep-end (I don’t know enough stuff yet), I think we’ll still get to dip our toes.\n\n\n\n\n\nI’ll just let the images speak.\n\n\n\n\n\ncozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful\n\n\n\n\n\n\n\nDune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.\n\n\n\n\n\n\n\ninspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space\n\n\n\n\nAll of the above were generated with simple but detailed text prompts.\n\n\n\n\n\n\nCaution\n\n\n\nThis blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you’re a bit rusty with some of the terms, I’ve defined the most common ones below.\n\n\n\n\n\n\n\n\nJargon\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nparameters\nthe values in the model that change what task it can do, updated through model training\n\n\narchitecture\nThe mathematical function that we pass input data and parameters to\n\n\nmodel\nThe combination of the architecture with a particular set of parameters\n\n\ngradient\nA Tensor that tells us how the vector field changes in any direction\n\n\nloss\nA measure of how good the model is, chosen to drive training via stochastic gradient descent\n\n\nCNN\nConvolutional Neural Network; a type of NN that works well for computer vision tasks\n\n\nembeddings\nA relatively-low dimension space into which we can translate high-dimensional vectors\n\n\nlatent\nA latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image\n\n\nnoise\nRandom variation of brightness or color information in images\n\n\n\n\n\n\nIf you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you’re like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment.  \nImagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit.  \nLet the probability be \\(P(X)\\) for an input Image X.\n\n\n\n\nflowchart LR\n  I1(X1) --&gt; F((Magic Function))\n  I2(X2) --&gt; F\n  I3(X3) --&gt; F\n  F --&gt; O1(\"P(X1)\")\n  F --&gt; O2(\"P(X2)\")\n  F --&gt; O3(\"P(X3)\")\n\n\n\n\n\n\n\n\nWait. How do we use this function to generate new digits?\nWe do the following steps:\n\n\nTry making a pixel of X darker and then check the output probability\nTry making a pixel of X lighter and then check probability\nDo this for every pixel in X (If X is a 28x28 image, that’s 784 times!)\n\nWhat we’re calculating is the probability of X being a handwritten digit wrt the gradient of X. This function is called the Score Funtion. \\[\\nabla_X P(X)\\]\nNoisy image = Clear Image + Noise\n\nFull disclaimer that this is an absurdly simplified explanation of it and the real thing is probably much more different and complicated but, it is a good place to build some intuition from.\n\nProbability of Noise:\n\n\n\n\n\n\nflowchart LR\n    B[\"Randomly selected sequential noise\"] --&gt;  X((+))\n    X --&gt; A[\"Somewhat noisy latent\"] --&gt; C((\"Diffusion Model\"))\n    C --&gt; D(\"Predicted Noise\") --&gt; - --&gt; A\n    A --&gt; E[\"Less noisy latent\"]\n    F[\"Randomly Selected Noise\"] --&gt; Z((+)) --&gt; E --&gt;\n    G((\"Updated Diffusion Model\")) --&gt; H(\"Predicted Noise\")\n    H --&gt; Y[-] --&gt; E\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nNotes\n\n\n\n\nUnet\nsomewhat noisy latents\nthe noise\n-\n\n\nVAE Decoder\nsmall latents tensor\nlarge image\nguided by captions\n\n\nCLIP text encoder\ntext\nembedding\n-"
  },
  {
    "objectID": "posts/stable-diffusion/index.html#cool-pics-to-pump-you-up",
    "href": "posts/stable-diffusion/index.html#cool-pics-to-pump-you-up",
    "title": "Stable Diffusion for dummies (written by a dummy)",
    "section": "",
    "text": "I’ll just let the images speak.\n\n\n\n\n\ncozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful\n\n\n\n\n\n\n\nDune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.\n\n\n\n\n\n\n\ninspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space\n\n\n\n\nAll of the above were generated with simple but detailed text prompts.\n\n\n\n\n\n\nCaution\n\n\n\nThis blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you’re a bit rusty with some of the terms, I’ve defined the most common ones below.\n\n\n\n\n\n\n\n\nJargon\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\nparameters\nthe values in the model that change what task it can do, updated through model training\n\n\narchitecture\nThe mathematical function that we pass input data and parameters to\n\n\nmodel\nThe combination of the architecture with a particular set of parameters\n\n\ngradient\nA Tensor that tells us how the vector field changes in any direction\n\n\nloss\nA measure of how good the model is, chosen to drive training via stochastic gradient descent\n\n\nCNN\nConvolutional Neural Network; a type of NN that works well for computer vision tasks\n\n\nembeddings\nA relatively-low dimension space into which we can translate high-dimensional vectors\n\n\nlatent\nA latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image\n\n\nnoise\nRandom variation of brightness or color information in images\n\n\n\n\n\n\nIf you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you’re like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment.  \nImagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit.  \nLet the probability be \\(P(X)\\) for an input Image X.\n\n\n\n\nflowchart LR\n  I1(X1) --&gt; F((Magic Function))\n  I2(X2) --&gt; F\n  I3(X3) --&gt; F\n  F --&gt; O1(\"P(X1)\")\n  F --&gt; O2(\"P(X2)\")\n  F --&gt; O3(\"P(X3)\")\n\n\n\n\n\n\n\n\nWait. How do we use this function to generate new digits?\nWe do the following steps:\n\n\nTry making a pixel of X darker and then check the output probability\nTry making a pixel of X lighter and then check probability\nDo this for every pixel in X (If X is a 28x28 image, that’s 784 times!)\n\nWhat we’re calculating is the probability of X being a handwritten digit wrt the gradient of X. This function is called the Score Funtion. \\[\\nabla_X P(X)\\]\nNoisy image = Clear Image + Noise\n\nFull disclaimer that this is an absurdly simplified explanation of it and the real thing is probably much more different and complicated but, it is a good place to build some intuition from.\n\nProbability of Noise:\n\n\n\n\n\n\nflowchart LR\n    B[\"Randomly selected sequential noise\"] --&gt;  X((+))\n    X --&gt; A[\"Somewhat noisy latent\"] --&gt; C((\"Diffusion Model\"))\n    C --&gt; D(\"Predicted Noise\") --&gt; - --&gt; A\n    A --&gt; E[\"Less noisy latent\"]\n    F[\"Randomly Selected Noise\"] --&gt; Z((+)) --&gt; E --&gt;\n    G((\"Updated Diffusion Model\")) --&gt; H(\"Predicted Noise\")\n    H --&gt; Y[-] --&gt; E\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nInput\nOutput\nNotes\n\n\n\n\nUnet\nsomewhat noisy latents\nthe noise\n-\n\n\nVAE Decoder\nsmall latents tensor\nlarge image\nguided by captions\n\n\nCLIP text encoder\ntext\nembedding\n-"
  },
  {
    "objectID": "calculation.html",
    "href": "calculation.html",
    "title": "Theory Threads and Dev Doodles",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data for time steps and corresponding noise levels\ntime_steps = np.arange(0, 1000, 1)  # Assuming 100 time steps\nnoise_levels =[-(x)**2 + 30*x + 2 for x in range(1,1001)] # Random noise levels between 0 and 1\n\n# Plotting the time-step noise schedule graph\nplt.figure(figsize=(10, 6))\nplt.plot(time_steps, noise_levels, color='b', label='Noise Levels')\nplt.xlabel('Time Steps')\nplt.ylabel('Noise Level')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey! I’m Harsh Sharma, a 20 yr old CS student with a new found interest for Deep learning. I love writing code and learning concepts that make me tear my hair out ; )\nI was introduced to the basics of HTML, CSS and some python in middle school and early high school, but I started programming much more seriously a few months before starting college.\nI also share a deep love for core CS concepts and the most recent project I did was related to writing my own little toy language. The languages I’m comfortable with include Python, C++, C, Javascript (and maybe Java). Other than that I’m also quite familar with HTML&CSS and am currently learning React as well.\nTech stuff apart, I love reading and listening to music. Current book of mine is The Republic by Plato and my favorite band atm is Nine Inch Nails (Ghosts I-IV 🤘)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Theory Threads and Dev Doodles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nStable Diffusion for dummies (written by a dummy)\n\n\n\n\n\n\n\nNotes\n\n\nFastai Part2\n\n\n\n\nA little informal guide for diffusion first timers.\n\n\n\n\n\n\nOct 29, 2023\n\n\nHarsh Sharma\n\n\n\n\n\n\nNo matching items"
  }
]