<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Harsh Sharma">
<meta name="dcterms.date" content="2023-10-31">
<meta name="description" content="An amateur’s guide to stable diffusion.">

<title>Theory Threads and Dev Doodles - Noisy to Nice</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Theory Threads and Dev Doodles</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Harsh5harma" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/HowdySharma" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Noisy to Nice</h1>
                  <div>
        <div class="description">
          An amateur’s guide to stable diffusion.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Notes</div>
                <div class="quarto-category">Fastai Part2</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Harsh Sharma </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 31, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#cool-ai-generated-images" id="toc-cool-ai-generated-images" class="nav-link" data-scroll-target="#cool-ai-generated-images"><span class="header-section-number">2</span> Cool AI generated images</a></li>
  <li><a href="#an-attempt-at-a-basic-image-generation-model" id="toc-an-attempt-at-a-basic-image-generation-model" class="nav-link" data-scroll-target="#an-attempt-at-a-basic-image-generation-model"><span class="header-section-number">3</span> An attempt at a basic image generation model</a></li>
  <li><a href="#problems-with-our-basic-model" id="toc-problems-with-our-basic-model" class="nav-link" data-scroll-target="#problems-with-our-basic-model"><span class="header-section-number">4</span> Problems with our basic model</a>
  <ul class="collapse">
  <li><a href="#vae-variational-auto-encoder" id="toc-vae-variational-auto-encoder" class="nav-link" data-scroll-target="#vae-variational-auto-encoder"><span class="header-section-number">4.1</span> VAE: Variational Auto Encoder</a></li>
  <li><a href="#clip" id="toc-clip" class="nav-link" data-scroll-target="#clip"><span class="header-section-number">4.2</span> CLIP</a></li>
  </ul></li>
  <li><a href="#updated-model" id="toc-updated-model" class="nav-link" data-scroll-target="#updated-model"><span class="header-section-number">5</span> Updated Model</a></li>
  <li><a href="#time-steps" id="toc-time-steps" class="nav-link" data-scroll-target="#time-steps"><span class="header-section-number">6</span> Time-Steps</a></li>
  <li><a href="#concluding-qna" id="toc-concluding-qna" class="nav-link" data-scroll-target="#concluding-qna"><span class="header-section-number">7</span> Concluding QNA</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">8</span> References</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="callout callout-style-default callout-caution callout-titled" title="About this blog">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About this blog
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This is my first ever blog and it’s on a topic I find relatively hard, although I’ve tried my best to be accurate in whatever I write, It won’t be surprising if this entire blog was riddled with inaccuracies and straight up incorrect stuff. I simply don’t have that necessary but aquired oversight yet. So I’m asking the reader to help me out with any errors they might spot and I’ll happily correct whatever it is that I presented wrong.</p>
</div>
</div>
</div>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Most people reading this have probably heard of image generation at some point in time. If you’re among those who haven’t, then this journey will be even better as I go in some fair detail to try and explain what it is.</p>
<p>If at any point, the things I talk about seem hard, don’t be discouraged as it’s more of a shock if you find this easy to read. Nevertheless, buckle up because although we might not go all the way into the deep-end (I don’t know enough stuff yet), I think we’ll still get to dip our toes.</p>
<p><img src="buckleup.gif" class="img-fluid" style="width:40.0%"> &nbsp;</p>
</section>
<section id="cool-ai-generated-images" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Cool AI generated images</h1>
<p>I’ll just let the images speak.<br>
<br>
</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img3.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">cozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img4.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Dune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img5.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">inspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p><br>
</p>
<p>All of the above were generated with simple but detailed text prompts. &nbsp; # Understanding the problem</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>This blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you’re a bit rusty with some of the terms, I’ve defined most used in this blog.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Jargon">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Jargon
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="table">
<colgroup>
<col style="width: 29%">
<col style="width: 70%">
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>parameters</td>
<td>The values in the model that change what task it can do, updated through model training</td>
</tr>
<tr class="even">
<td>architecture</td>
<td>The mathematical function that we pass input data and parameters to</td>
</tr>
<tr class="odd">
<td>model</td>
<td>The combination of the architecture with a particular set of parameters</td>
</tr>
<tr class="even">
<td>gradient</td>
<td>A Tensor that tells us how the vector field changes in any direction</td>
</tr>
<tr class="odd">
<td>loss</td>
<td>A measure of how good the model is, chosen to drive training via stochastic gradient descent</td>
</tr>
<tr class="even">
<td>metric</td>
<td>A human readable way to measure the performance of a model</td>
</tr>
<tr class="odd">
<td>CNN</td>
<td>Convolutional Neural Network; a type of NN that works well for computer vision tasks</td>
</tr>
<tr class="even">
<td>embeddings</td>
<td>A relatively-low dimension space into which we can translate high-dimensional vectors</td>
</tr>
<tr class="odd">
<td>latent</td>
<td>A latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image</td>
</tr>
<tr class="even">
<td>latent space</td>
<td>A feature or embedding space where similar embeddings lie close to each other, and different farther.</td>
</tr>
<tr class="odd">
<td>noise</td>
<td>Random variation of brightness or color information in images</td>
</tr>
<tr class="even">
<td>U-Net</td>
<td>Deep Learning architecture used for classifying each pixel in an image into a category or class</td>
</tr>
<tr class="odd">
<td>VAE</td>
<td>Variational Auto Encoder</td>
</tr>
<tr class="even">
<td>CLIP</td>
<td>Contrastive Language-Image Pre-Training, a visual classification NN</td>
</tr>
<tr class="odd">
<td>multimodal learning</td>
<td>A type of learning where the model is trained to understand and work with multiple forms of input data</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>If you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you’re like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment. &nbsp;</p>
<p>Imagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit. &nbsp;</p>
<p>Let the probability be <span class="math inline">\(P(X)\)</span> for an input Image X.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  I1(X1) --&gt; F((Magic Function))
  I2(X2) --&gt; F
  I3(X3) --&gt; F
  F --&gt; O1("P(X1)")
  F --&gt; O2("P(X2)")
  F --&gt; O3("P(X3)")

</pre>
</div>
</div>
</div>
</div>
<p><br>
</p>
<p>Wait. How do we use this function to generate new digits?<br>
We do the following steps:<br>
</p>
<ol type="1">
<li>Try making a pixel of X darker and then check the output probability</li>
<li>Try making a pixel of X lighter and then check probability</li>
<li>Do this for every pixel in X (If X is a 28x28 image, that’s 784 times!)</li>
</ol>
<p>In the above steps, we’re calculating the change in probability of the images being handwritten wrt. to each pixel in the image (784 for a 28x28 image), mathematically it can be represented as follows: <span class="math display">\[\nabla_X P(X)\]</span><br>
</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(\nabla_X P(X)\)</span> is itself a vector of size 784.</p>
</div>
</div>
<p>Next step we do is to multiply <span class="math inline">\(\nabla_X P(X)\)</span> with some constant. <span class="math display">\[c*\nabla_X P(X)\]</span></p>
<p>Now we subtract the above term from the 784 pixel values of <span class="math inline">\(X\)</span> and in turn increase the output probability, and we do it a bunch of times.<br>
</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  I(X) --&gt; F((Magic Function))
  F --&gt; O("P(X)")
  O --&gt; G["c * ∇ₓP(X)"]
  G --&gt; S["-"]
  S --&gt; I
</pre>
</div>
</div>
</div>
</div>
<p><br>
</p>
<p>Having gone through all that, it would be an intelligent guess to say that now all we need is for someone to give us the magic function so we can start generating.<br>
</p>
<p>But there’s some bad news, nobody is going to hand us any magical function that takes care of everything under the hood. We’re going to have to train our own neural network on a lot of handwritten data to do this for us.<br>
</p>
<p>Seems simple enough to do for any seasoned DL practitioner. Alas! There’s a catch. Now I don’t know the why or how, but smarter people than me figured out that it’s pretty much practically impossible if not insanely hard to create a metric that tells us how much our image looks like a specific digit.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Don’t forget that our end goal is making super pretty images like we saw at the start, not some basic digits.</p>
</div>
</div>
<p>So, they decided on an interesting but very logical approach.<br>
</p>
</section>
<section id="an-attempt-at-a-basic-image-generation-model" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> An attempt at a basic image generation model</h1>
<p>Every image we train on can be overlayed with some random amounts of noise, the result would be a noisier image of course but now we have some place to start with.<br>
<code>Noisy Image = Noise + Clear Image</code></p>
<p>Wkt we can’t predict how much an image looks like something, but turns out we can try and predict how noisy an image is, and then using basic arithmetic we can see that we’d just have to subtract that predicted noise from the noisy image to end up with a relatively clear Image.<br>
</p>
<p>Passing in a starting pic and repeating this over and over would train our model to get good at predicting the noise in an image and getting rid of it.<br>
</p>
<p>And once it has trained over enough inputs, it can generate new images for us because it now knows how to denoise inputs.</p>
<p>So we can use a model that works roughly in the fashion shown below:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  I["Input Image"] == "(1)" ==&gt; M(("U-Net"))
  M == "(2)" ==&gt; O["Predicted Noise"]
  O == "(3)" ==&gt; Z["Subtracted from"]
  Z == "(4)" ==&gt; I
  I == "(5)" ==&gt; nI["Less noisy image"]
  nI == "(6)" ==&gt; M
</pre>
</div>
</div>
</div>
</div>
<p><br>
</p>
<p>Although the model above is decent, it can be optimised further using the concept of <em>guidance</em>. I’ll explain.<br>
</p>
<p>Currently, at the start of the training process, we’re leaving our model in the dark really with no clue about what digit it’s supposed to identify since we’re passing in somewhat noisy inputs.<br>
</p>
<p>Say our model knew what it was supposed to look for, wouldn’t it make logical sense then to pass in that thing? That’s what guidance does. Along with the input (obviously), we also pass the digit our model is supposed to remove noise from.<br>
</p>
<p>Now once the model is trained, <em>guidance</em> would help generate better new images, because it can denoise better! Duh. So our current model would actually end up looking more like:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  I["Input Image"] == "(1)" ==&gt; M(("U-Net"))
  G["OHE Guidance Vector"] == "(1, 6)" ==&gt; M
  M == "(2)" ==&gt; O["Predicted Noise"]
  O == "(3)" ==&gt; Z["Subtracted from"]
  Z == "(4)" ==&gt; I
  I == "(5)" ==&gt; nI["Less noisy image"]
  nI == "(6)" ==&gt; M
</pre>
</div>
</div>
</div>
</div>
</section>
<section id="problems-with-our-basic-model" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Problems with our basic model</h1>
<p>If we were living in the 90s, the model we built would certainly grab a lot of attention and maybe fame, but we’re not. We have overlooked some very grave issues.</p>
<ol type="1">
<li>We’re not working with tiny <span class="math inline">\(28x28\)</span> pixel images of handwritten digits here, we’re working with <span class="math inline">\(512x512x3\)</span> (High quality RGB) images.</li>
<li>Each of our 9 handwritten digits can be one-hot encoded into a vector but beyond that, we can’t one-hot encode infinite possible texts.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Try one hot encoding any phrase of your choice. It’s hard.</p>
</div>
</div>
<p>Well then, is our case hopeless here? Were the images I showed faked? No, for we have the <code>VAE autoencoder</code> and the <code>CLIP</code> multimodal encoders and decoders to the rescue. Let’s understand what they are and get into how they solve both of our problems.<br>
</p>
<section id="vae-variational-auto-encoder" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="vae-variational-auto-encoder"><span class="header-section-number">4.1</span> VAE: Variational Auto Encoder</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="VAE.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Variational Auto Encoder</figcaption>
</figure>
</div>
<p><br>
In the context of our model, VAE is just a data compression and decompression algorithm with a fancy name.&nbsp;</p>
<p>VAE tries that what came in as input is the same as what comes out, it’s architecture might look similar to U-Net but VAE does not have cross connections unlike the former and VAE is also an autoencoder.&nbsp;</p>
<p>The special thing about autoencoders is that they can split in half and we can just use the encoder and decoder seperately.</p>
<p>The encoder serves as a great compression method and on the <span class="math inline">\(512*512*3\)</span> example it reduces the pixel count from <span class="math inline">\(786,432\)</span> to a mere <span class="math inline">\(16,384\)</span>. A size reduction of <span class="math inline">\(48\)</span> times.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The encoder peforms compression by doing stride 2 convolutions and doubling the channel size, this helps in increasing the complexity and expressive power of the model without increasing the spatial feature maps.</p>
<p>So, a stride 2 convolution reduces the spatial dimensions and memory usage and additional channels give the model more capacity to learn.</p>
</div>
</div>
<p>And if we have a copy of the decoder, we can feed the encoded latent to it and end up with the high resolution image again.</p>
<p>This model serves a great compression algorithm because it’s been trained on millions upon millions of input images, at the start of its training process, the VAE spits out random noise out of it’s decoder but if we use the humble <code>Mean Squared Error</code> function on the encoder input and decoder output to train the VAE reduce the loss.</p>
<p>How it works is interesting though. VAE, is built up of two main components and each of them can work independently once the autoencoder has been trained.<br>
</p>
<p>The first part consists of the Encoder. This is where compression happens, we start with a high resolution image of <span class="math inline">\(512*512*3\)</span> and keep performing convolutions of stride 2 and doubling the channels till we end up with a relatively much lower dimension image. This image is called the <code>latent</code> and the multi-dimensional space it exists in is called the <code>latent space</code>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A <code>latent</code> contains the most interesting and useful information of an image.</p>
<p>A <code>channel</code> in a CNN refers to a specific component of the image data, in the case of a colour image, the channels typically represent different colors.</p>
<p>A <code>feature</code> in CNN refers to specific characteristics or pattern in the input data that the network learns to recognise. These patterns can be simple or complex, early layers of a CNN features are basic patterns and deeper in the network, they’re more abstract and complex.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A stride two convolution means that at each step, the convolution skips a column and a row. So for a <span class="math inline">\(4*4\)</span> grid, a stride 2 convolution would bring the size down</p>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  A["High Res Image"] --&gt; M(("VAE Encoder"))
  M --&gt; O["Image Latent Tensor"]
</pre>
</div>
</div>
</div>
</div>
<p>Now let’s get back to what we need VAE for, the size problems our older model had. Since we can compress image sizes by godly amounts, there’s no doubt, that we’ll train the U-Net model with these encoded images (latents).</p>
<p>So, if our U-Net needs to be trained on 10 million images, we’ll pass each one of them through <code>VAE's encoder</code> first, and then we feed these latents into the U-Net 100s to 1000s of times.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Don’t forget that U-Net denoises input images through repetition.</p>
</div>
</div>
<p>Now our model looks like this:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  I["Noisy Latent Tensor"] == "(1)" ==&gt; M(("U-Net"))
  G["OHE Guidance Vector"] == "(1, 6)" ==&gt; M
  M == "(2)" ==&gt; O["Predicted Noise"]
  O == "(3)" ==&gt; Z["Subtracted From"]
  Z == "(4)" ==&gt; I
  I == "(5)" ==&gt; nI["Less Noisy Latent Tensor"]
  nI == "(6)" ==&gt; M
</pre>
</div>
</div>
</div>
</div>
<p>Wouldn’t this new latent based model spit out latents too? Yes, it will but we don’t forget that we also have the other half of our autoencoder. The <code>VAE decoder</code> will be able to take in the generated latent as input and then output a high resolution image. It’d be like there wasn’t any compression-decompression to begin with.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  A["U-Net Output Latent"] --&gt; M(("VAE Decoder"))
  M --&gt; O["High Res Version of Generated Image"]
</pre>
</div>
</div>
</div>
</div>
<p>Technically we don’t need to use an autoencoder, but it greatly saves on computation necessary and the time it takes to generate an image as well as the modern training itself.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>VAE encoder</code>: Used during model training.</p>
<p><code>VAE decoder</code>: Used during model inference.</p>
</div>
</div>
</section>
<section id="clip" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="clip"><span class="header-section-number">4.2</span> CLIP</h2>
<p>Ignore the weird name for now and focus on the second problem we had. Which was, finding a way to create embeddings for any possible text and not just the digits b/w 0 to 9 so we could guide the model.</p>
<p>What if we had a model that spat out a vector representation of every complex sentence, such that we could pass it with our <code>latent tensor</code> as <code>guidance</code>. And what if, that model created textual embeddings in such a way, that they ended up being close to the <code>latent tensor</code>’s embedding in the <code>latent space</code>.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="My personal intuition for this, could be wrong.">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My personal intuition for this, could be wrong.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If the above paragraph was hard to get, I can explain with a more grounded example. I’m aware that the human brain and neural networks are not at all the same in the manner with which they function, and are designed but I couldn’t think of an easier example, so bear with me.</p>
<p>Imagine a <em>guitar with a few broken strings</em>, after reading this text could your brain think up of an image showing the same? It might not be super clear, 4k quality or something, but you could in your head think of an image that had the most important features that a <em>guitar with a few broken strings</em> might have.</p>
<p>I think that a <code>latent space</code> is just that neural network analog for that headspace we go into. Our brain was able to process and break-down the text and then also match it to a visual representation.</p>
<p>So we essentially need a model, that is capable of doing something similar to this in essence. CLIP is one such model that does this job.</p>
</div>
</div>
</div>
<p>CLIP is made up of two models. A Text-encoder and an Image-encoder.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart TD
  T["Input text"] --&gt; TE(["CLIP Text Encoder"])
  I["Input Image"] --&gt; IE(["CLIP Image Encoder"])
  TE --&gt; VT["Vector Representation of Text"]
  IE --&gt; VI["Vector Representation of Image"]
  VT --&gt; LS(("Latent Space"))
  VI --&gt; LS
</pre>
</div>
</div>
</div>
</div>
<p>Both encoders need to be trained first with randomly selected weights. The ‘CL’ in CLIP stands for <code>contrastive loss</code> and that’s what helps in training both the text and image encoders. Fancy name apart, training process is quite simple.</p>
<p>Let’s take three random text prompts:</p>
<p><code>Text 1</code>: <em>Muscular Hen’s caricature</em></p>
<p><code>Text 2</code>: <em>Broken Egg</em></p>
<p><code>Text 3</code>: <em>Senior man with long hair and beard portrait</em></p>
<p>We’ll call the vector representation of each text prompt as <code>T1</code>, <code>T2</code>, <code>T3</code> respectively.</p>
<p>We also now have 3 images of the same, and we’ll use <code>I1</code>, <code>I2</code>, <code>I3</code> to represent the vector representations of these images. All of these vector representations were generated by throwing them into the CLIP encoders (and they’re random right now because we haven’t trained CLIP yet).</p>
<p>Now, we can make a grid with the Image vectors lined up along the columns and the Text vectors lined along the rows.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="CLIP.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Training Grid</figcaption>
</figure>
</div>
<p>Each cell of the grid contains the dot product of the two representation vectors. The dot product of vectors that actually describe an image with the right text will be the highest, i.e.&nbsp;of the diagonal cells.</p>
<p>The non-diagonal cells on the other hand, should have low values once the model is trained, because the embeddings are far apart from each other in the <code>latent-space</code>. The text representation vector is not correct for the image.</p>
<p>Now as the final step, we sum up all the diagonals together and we also sum up all the non-diagonals separately.</p>
<ol type="1">
<li><span class="math inline">\(Sum_d = \text{Sum of Diagonals}\)</span></li>
<li><span class="math inline">\(Sum_nd = \text{Sum of Non-Diagonals}\)</span></li>
<li><span class="math inline">\(Loss = Sum_d - Sum_nd\)</span></li>
</ol>
<p>The loss function is known as <code>Contrastive Loss</code>, and it should be zero if all our generated embeddings are perfect matches.</p>
<p>So, to conclude this section. CLIP is a multi-modal model as our embeddings all share the same space and it helps us solve our second problem of not having a way to create embeddings for complex text.</p>
</section>
</section>
<section id="updated-model" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Updated Model</h1>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  I["Noisy Latent Tensor"] == "(1)" ==&gt; M(("U-Net"))
  G["CLIP Generated Embedding"] == "(1, 6)" ==&gt; M
  M == "(2)" ==&gt; O["Predicted Noise"]
  O == "(3)" ==&gt; Z["Subtracted From"]
  Z == "(4)" ==&gt; I
  I == "(5)" ==&gt; nI["Less Noisy Latent Tensor"]
  nI == "(6)" ==&gt; M
</pre>
</div>
</div>
</div>
</div>
<p>Now our model can take any image as input with any arbritarily complex text describing what the image is about and our model would be able to use that as guidance to denoise input latents better.</p>
<p>And what about generation? For image generation, our model could take as input any random bits of noise and convert that noise into something that matches the text embedding by using the feature vectors of the very same embedding.</p>
<p>This model could serve as the final one, but in practice stable diffusion has one extra implementation detail that we haven’t discussed yet. It’s called Time-Steps.</p>
</section>
<section id="time-steps" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Time-Steps</h1>
<p>To start off, time steps has nothing to do with time. It’s just an overhang from the math of the first few papers written on stable defusion.</p>
<p>It’s just a method of generating random noise for the inputs we pass in to our model. The noise used is varying and the <code>noise schedule</code> is some monotonically decreasing function.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="noise.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Time-Steps v/s noise schedule</figcaption>
</figure>
</div>
<p>The algorithm to pick noise is:</p>
<ol type="1">
<li>Randomly choose a number b/w 1-100</li>
<li>Look at noise schedule for that number.</li>
</ol>
<p><code>t=1</code> has a lot of noise, 1000 has the least. So the time-step <code>t</code> is pretty much telling us how much noise to use.</p>
<p>How is this used in practice? Well, we generate noise using the 2 steps above for each image in our mini-batch and we add that generated noise to each image in the mini-batch respectively.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>These days, it’s widely accepted that the addition of noise to input images using time-steps in stable-diffusion impacted the quality of the generated images negatively and it’s not used in newer diffusion models.</p>
</div>
</div>
</section>
<section id="concluding-qna" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Concluding QNA</h1>
<p>How does whatever we did above tie in with the first equation we had?</p>
<p><code>Some Noisy Image</code> - <span class="math inline">\(c*\nabla_X P(X)\)</span></p>
<ol type="1">
<li><dl>
<dt>What would the value of <span class="math inline">\(c\)</span> be and is it similar to learning rate?</dt>
<dd>
<code>Ans</code>. The value of <span class="math inline">\(c\)</span> depends on the diffusion sampler we use and yes <span class="math inline">\(c\)</span> is kind of like a learning rate.
</dd>
</dl></li>
<li><dl>
<dt>Why can’t our model jump to the best image in one step?</dt>
<dd>
<code>Ans</code>. Because things that look weird and hideous like the image below don’t appear in our training set, and our model doesn’t know what to do with them.
</dd>
</dl></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ugly.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">hideous image generated by the model</figcaption>
</figure>
</div>
<p>Our model only knows how to deal with somewhat noisy images and that’s it. It can only denoise whenever it sees some noise.</p>
<p>That’s why we subtract just a bit of noise (hence the <span class="math inline">\(c\)</span>), so that we are always left with some noisy latent.</p>
</section>
<section id="references" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> References</h1>
<ol type="1">
<li>fastai practical deep learning for coders</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>