{"title":"Stable Diffusion for dummies (written by a dummy)","markdown":{"yaml":{"title":"Stable Diffusion for dummies (written by a dummy)","description":"A little informal guide for diffusion first timers.","author":"Harsh Sharma","date":"10/29/2023","image":"logo.png","categories":["Notes","Fastai Part2"]},"headingText":"Stable Diffusion for dummies","containsRefs":false,"markdown":"\n\nMost people reading this have probably heard of image generation at some point in time. If you're among those who haven't, then this journey will be even better as I go in some fair detail to try and explain what it is.\n\nIf at any point, the things I talk about seem hard, don't be discouraged as it's more of a shock if you find this easy to read. Nevertheless, buckle up because although we might not go all the way into the deep-end (I don't know enough stuff yet), I think we'll still get to dip our toes.\n\n![](buckleup.gif){width=40%}\n\n## Cool pics to pump you up:\nI'll just let the images speak.  \n\\\n\n::: {layout=\"[[1,1], [1]]\"}\n![cozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful](img3.jpg){width=50%}\n\n![Dune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.](img4.jpg){width=50%}\n\n![inspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space](img5.jpg)\n:::\n\\\n\nAll of the above were generated with simple but detailed text prompts.\n\n::: {.callout-caution}\nThis blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you're a bit rusty with some of the terms, I've defined most used in this blog.\n:::\n::: {.callout-note collapse='true' title='Jargon' }\n|Term | Meaning |\n|-----|------------|\n|parameters| the values in the model that change what task it can do, updated through model training|\n| architecture | The mathematical function that we pass input data and parameters to |\n|model | The combination of the architecture with a particular set of parameters|  \n|gradient | A Tensor that tells us how the vector field changes in any direction |\n| loss | A measure of how good the model is, chosen to drive training via stochastic gradient descent|\n|metric| A human readable way to measure the performance of a model |\n|CNN| Convolutional Neural Network; a type of NN that works well for computer vision tasks | \n| embeddings | A relatively-low dimension space into which we can translate high-dimensional vectors|\n| latent | A latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image|\n|latent space| A feature or embedding space where similar embeddings lie close to each other, and different farther.|\n|noise| Random variation of brightness or color information in images|\n|U-Net| \n|VAE| Variational Auto Encoder|\n|CLIP| Contrastive Language-Image Pre-Training, a visual classification NN|\n:::\n\nIf you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you're like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment.\n\\ \n\nImagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit. \n\\ \n\nLet the probability be $P(X)$ for an input Image X.\n```{mermaid}\nflowchart LR\n  I1(X1) --> F((Magic Function))\n  I2(X2) --> F\n  I3(X3) --> F\n  F --> O1(\"P(X1)\")\n  F --> O2(\"P(X2)\")\n  F --> O3(\"P(X3)\")\n\n```\n\n\\\n\nWait. How do we use this function to generate new digits?\\\nWe do the following steps:\\\n\n1. Try making a pixel of X darker and then check the output probability\n2. Try making a pixel of X lighter and then check probability\n3. Do this for every pixel in X (If X is a 28x28 image, that's 784 times!)\n\nIn the above steps, we're calculating the change in probability of the images being handwritten wrt. to each pixel in the image (784 for a 28x28 image), mathematically it can be represented as follows: \n$$\\nabla_X P(X)$$\n\\\n\n::: {.callout-note}\n$\\nabla_X P(X)$ is itself a vector of size 784.\n:::\n\nNext step we do is to multiply $\\nabla_X P(X)$ with some constant.\n$$c*\\nabla_X P(X)$$\n\nNow we subtract the above term from the 784 pixel values of $X$ and in turn increase the output probability, and we do it a bunch of times.\\\n\n```{mermaid}\nflowchart LR\n  I(X) --> F((Magic Function))\n  F --> O(\"P(X)\")\n  O --> G[\"c * ∇ₓP(X)\"]\n  G --> S[\"-\"]\n  S --> I\n```\n\\\n\nHaving gone through all that, it would be an intelligent guess to say that now all we need is for someone to give us the magic function so we can start generating.\\\n\nBut there's some bad news, nobody is going to hand us any magical function that takes care of everything under the hood. We're going to have to train our own neural network on a lot of handwritten data to do this for us.\\\n\nSeems simple enough to do for any seasoned DL practitioner. Alas! There's a catch. Now I don't know the why or how, but smarter people than me figured out that it's pretty much practically impossible if not insanely hard to create a metric that tells us how much our image looks like a specific digit. \n\n:::{.callout-important}\nDon't forget that our end goal is making super pretty images like we saw at the start, not some basic digits.\n:::\n\\\n\nSo, they decided on an interesting but very logical approach.\\\n\nEvery image we train on can be overlayed with some random amounts of noise, the result would be a noisier image of course but now we have some place to start with.\\\n`Noisy Image = Noise + Clear Image`\n\nWkt we can't predict how much an image looks like something, but turns out we can try and predict how noisy an image is, and then using basic arithmetic we can see that we'd just have to subtract that predicted noise from the noisy image to end up with a relatively Clear Image. \\\n\nPassing in a starting pic and repeating this over and over would train our model to get good at predicting the noise in an image and getting rid of it. \\\n\nThis model is very similar to a U-Net\n\n## Specifics of Training\n\n\nProbability of Noise:\\\n\n```{mermaid}\nflowchart LR\n    B[\"Randomly selected sequential noise\"] -->  X((+))\n    X --> A[\"Somewhat noisy latent\"] --> C((\"Diffusion Model\"))\n    C --> D(\"Predicted Noise\") --> - --> A\n    A --> E[\"Less noisy latent\"]\n    F[\"Randomly Selected Noise\"] --> Z((+)) --> E -->\n    G((\"Updated Diffusion Model\")) --> H(\"Predicted Noise\")\n    H --> Y[-] --> E\n```\n\n\\\n\n| Model             | Input                  | Output      | Notes              |\n| ----------------- | ---------------------- | ----------- | ------------------ |\n| Unet              | somewhat noisy latents | the noise   | -                  |\n| VAE Decoder       | small latents tensor   | large image | guided by captions |\n| CLIP text encoder | text                   | embedding   | -                  |\n","srcMarkdownNoYaml":"\n# Stable Diffusion for dummies\n\nMost people reading this have probably heard of image generation at some point in time. If you're among those who haven't, then this journey will be even better as I go in some fair detail to try and explain what it is.\n\nIf at any point, the things I talk about seem hard, don't be discouraged as it's more of a shock if you find this easy to read. Nevertheless, buckle up because although we might not go all the way into the deep-end (I don't know enough stuff yet), I think we'll still get to dip our toes.\n\n![](buckleup.gif){width=40%}\n\n## Cool pics to pump you up:\nI'll just let the images speak.  \n\\\n\n::: {layout=\"[[1,1], [1]]\"}\n![cozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful](img3.jpg){width=50%}\n\n![Dune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.](img4.jpg){width=50%}\n\n![inspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space](img5.jpg)\n:::\n\\\n\nAll of the above were generated with simple but detailed text prompts.\n\n::: {.callout-caution}\nThis blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you're a bit rusty with some of the terms, I've defined most used in this blog.\n:::\n::: {.callout-note collapse='true' title='Jargon' }\n|Term | Meaning |\n|-----|------------|\n|parameters| the values in the model that change what task it can do, updated through model training|\n| architecture | The mathematical function that we pass input data and parameters to |\n|model | The combination of the architecture with a particular set of parameters|  \n|gradient | A Tensor that tells us how the vector field changes in any direction |\n| loss | A measure of how good the model is, chosen to drive training via stochastic gradient descent|\n|metric| A human readable way to measure the performance of a model |\n|CNN| Convolutional Neural Network; a type of NN that works well for computer vision tasks | \n| embeddings | A relatively-low dimension space into which we can translate high-dimensional vectors|\n| latent | A latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image|\n|latent space| A feature or embedding space where similar embeddings lie close to each other, and different farther.|\n|noise| Random variation of brightness or color information in images|\n|U-Net| \n|VAE| Variational Auto Encoder|\n|CLIP| Contrastive Language-Image Pre-Training, a visual classification NN|\n:::\n\nIf you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you're like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment.\n\\ \n\nImagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit. \n\\ \n\nLet the probability be $P(X)$ for an input Image X.\n```{mermaid}\nflowchart LR\n  I1(X1) --> F((Magic Function))\n  I2(X2) --> F\n  I3(X3) --> F\n  F --> O1(\"P(X1)\")\n  F --> O2(\"P(X2)\")\n  F --> O3(\"P(X3)\")\n\n```\n\n\\\n\nWait. How do we use this function to generate new digits?\\\nWe do the following steps:\\\n\n1. Try making a pixel of X darker and then check the output probability\n2. Try making a pixel of X lighter and then check probability\n3. Do this for every pixel in X (If X is a 28x28 image, that's 784 times!)\n\nIn the above steps, we're calculating the change in probability of the images being handwritten wrt. to each pixel in the image (784 for a 28x28 image), mathematically it can be represented as follows: \n$$\\nabla_X P(X)$$\n\\\n\n::: {.callout-note}\n$\\nabla_X P(X)$ is itself a vector of size 784.\n:::\n\nNext step we do is to multiply $\\nabla_X P(X)$ with some constant.\n$$c*\\nabla_X P(X)$$\n\nNow we subtract the above term from the 784 pixel values of $X$ and in turn increase the output probability, and we do it a bunch of times.\\\n\n```{mermaid}\nflowchart LR\n  I(X) --> F((Magic Function))\n  F --> O(\"P(X)\")\n  O --> G[\"c * ∇ₓP(X)\"]\n  G --> S[\"-\"]\n  S --> I\n```\n\\\n\nHaving gone through all that, it would be an intelligent guess to say that now all we need is for someone to give us the magic function so we can start generating.\\\n\nBut there's some bad news, nobody is going to hand us any magical function that takes care of everything under the hood. We're going to have to train our own neural network on a lot of handwritten data to do this for us.\\\n\nSeems simple enough to do for any seasoned DL practitioner. Alas! There's a catch. Now I don't know the why or how, but smarter people than me figured out that it's pretty much practically impossible if not insanely hard to create a metric that tells us how much our image looks like a specific digit. \n\n:::{.callout-important}\nDon't forget that our end goal is making super pretty images like we saw at the start, not some basic digits.\n:::\n\\\n\nSo, they decided on an interesting but very logical approach.\\\n\nEvery image we train on can be overlayed with some random amounts of noise, the result would be a noisier image of course but now we have some place to start with.\\\n`Noisy Image = Noise + Clear Image`\n\nWkt we can't predict how much an image looks like something, but turns out we can try and predict how noisy an image is, and then using basic arithmetic we can see that we'd just have to subtract that predicted noise from the noisy image to end up with a relatively Clear Image. \\\n\nPassing in a starting pic and repeating this over and over would train our model to get good at predicting the noise in an image and getting rid of it. \\\n\nThis model is very similar to a U-Net\n\n## Specifics of Training\n\n\nProbability of Noise:\\\n\n```{mermaid}\nflowchart LR\n    B[\"Randomly selected sequential noise\"] -->  X((+))\n    X --> A[\"Somewhat noisy latent\"] --> C((\"Diffusion Model\"))\n    C --> D(\"Predicted Noise\") --> - --> A\n    A --> E[\"Less noisy latent\"]\n    F[\"Randomly Selected Noise\"] --> Z((+)) --> E -->\n    G((\"Updated Diffusion Model\")) --> H(\"Predicted Noise\")\n    H --> Y[-] --> E\n```\n\n\\\n\n| Model             | Input                  | Output      | Notes              |\n| ----------------- | ---------------------- | ----------- | ------------------ |\n| Unet              | somewhat noisy latents | the noise   | -                  |\n| VAE Decoder       | small latents tensor   | large image | guided by captions |\n| CLIP text encoder | text                   | embedding   | -                  |\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"slate","title-block-banner":true,"title":"Stable Diffusion for dummies (written by a dummy)","description":"A little informal guide for diffusion first timers.","author":"Harsh Sharma","date":"10/29/2023","image":"logo.png","categories":["Notes","Fastai Part2"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}