{"title":"Stable Diffusion for dummies (written by a dummy)","markdown":{"yaml":{"title":"Stable Diffusion for dummies (written by a dummy)","description":"A little informal guide for diffusion first timers.","author":"Harsh Sharma","date":"10/29/2023","image":"logo.png","categories":["Notes","Fastai Part2"]},"headingText":"Stable Diffusion for dummies","containsRefs":false,"markdown":"\n\nMost people reading this have probably heard of image generation at some point in time. If you're among those who haven't, then this journey will be even better as I go in some fair detail to try and explain what it is.\n\nIf at any point, the things I talk about seem hard, don't be discouraged as it's more of a shock if you find this easy to read. Nevertheless, buckle up because although we might not go all the way into the deep-end (I don't know enough stuff yet), I think we'll still get to dip our toes.\n\n![](buckleup.gif)\n\n## Cool pics to pump you up:\n\n\\\n\nI'll just let the images speak.  \n\n\\\n\n![cozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful](img3.jpg)\n\n\\\n\n![Dune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.](img4.jpg)\n\n\\\n\n![inspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space](img5.jpg)\n\n\\\n\nAll of the above were generated with simple but detailed text prompts.\n\n::: {.callout-caution}\nThis blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you're a bit rusty with some of the terms, I've defined the most common ones below.\n:::\n::: {.callout-note collapse=\"true\" title='Jargon'}\n|Term | Meaning |\n|-----|------------|\n|parameters| the values in the model that change what task it can do, updated through model training|\n| architecture | The mathematical function that we pass input data and parameters to |\n|model | The combination of the architecture with a particular set of parameters|  \n|gradient | A Tensor that tells us how the vector field changes in any direction |\n| loss | A measure of how good the model is, chosen to drive training via stochastic gradient descent|\n|CNN| Convolutional Neural Network; a type of NN that works well for computer vision tasks | \n| embeddings | A relatively-low dimension space into which we can translate high-dimensional vectors|\n| latent | A latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image|\n|noise| Random variation of brightness or color information in images|\n:::\n\nIf you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you're like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment.\n\\ \n\nImagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit. \n\\ \n\nLet the probability be $P(X)$ for an input Image X.\n```{mermaid}\nflowchart LR\n  I1(X1) --> F((Magic Function))\n  I2(X2) --> F\n  I3(X3) --> F\n  F --> O1(\"P(X1)\")\n  F --> O2(\"P(X2)\")\n  F --> O3(\"P(X3)\")\n\n```\n\n\\\n\nWait. How do we use this function to generate new digits?\\\nWe do the following steps:\\\n\n1. Try making a pixel of X darker and then check the output probability\n2. Try making a pixel of X lighter and then check probability\n3. Do this for every pixel in X (If X is a 28x28 image, that's 784 times!)\n\nWhat we're calculating is the probability of X being a handwritten digit wrt the gradient of X. This function is called the `Score Funtion`.\n$$\\nabla_X P(X)$$\n\n`Noisy image = Clear Image + Noise`\\\n\nFull disclaimer that this is an absurdly simplified explanation of it and the real thing is probably much more different and complicated but, it is a good place to build some intuition from.\n\\\n\nProbability of Noise:\n\n\n\n\n\n\\\n\n```{mermaid}\nflowchart LR\n    B[\"Randomly selected sequential noise\"] -->  X((+))\n    X --> A[\"Somewhat noisy latent\"] --> C((\"Diffusion Model\"))\n    C --> D(\"Predicted Noise\") --> - --> A\n    A --> E[\"Less noisy latent\"]\n    F[\"Randomly Selected Noise\"] --> Z((+)) --> E -->\n    G((\"Updated Diffusion Model\")) --> H(\"Predicted Noise\")\n    H --> Y[-] --> E\n```\n\n\\\n\n| Model             | Input                  | Output      | Notes              |\n| ----------------- | ---------------------- | ----------- | ------------------ |\n| Unet              | somewhat noisy latents | the noise   | -                  |\n| VAE Decoder       | small latents tensor   | large image | guided by captions |\n| CLIP text encoder | text                   | embedding   | -                  |\n","srcMarkdownNoYaml":"\n# Stable Diffusion for dummies\n\nMost people reading this have probably heard of image generation at some point in time. If you're among those who haven't, then this journey will be even better as I go in some fair detail to try and explain what it is.\n\nIf at any point, the things I talk about seem hard, don't be discouraged as it's more of a shock if you find this easy to read. Nevertheless, buckle up because although we might not go all the way into the deep-end (I don't know enough stuff yet), I think we'll still get to dip our toes.\n\n![](buckleup.gif)\n\n## Cool pics to pump you up:\n\n\\\n\nI'll just let the images speak.  \n\n\\\n\n![cozy warm image of an indian male programmer sitting near a beautiful pond, scenic, matches white background, beautiful](img3.jpg)\n\n\\\n\n![Dune arakis in the background. Paul Atreides giving speech. Crowd listening is charged, ominous foreboding theme.](img4.jpg)\n\n\\\n\n![inspiring landscape poster showing humanity as a multiplanetary species, sprawling metropolis with large structures, awe-inspiring, 1920x1080 image, outer space](img5.jpg)\n\n\\\n\nAll of the above were generated with simple but detailed text prompts.\n\n::: {.callout-caution}\nThis blog and deep learning in general have the tendency to contain lots of technical jargon. It might be something stupidly simple once you actually learn what it is, but for some reason the DL community likes cool names I guess. In case you're a bit rusty with some of the terms, I've defined the most common ones below.\n:::\n::: {.callout-note collapse=\"true\" title='Jargon'}\n|Term | Meaning |\n|-----|------------|\n|parameters| the values in the model that change what task it can do, updated through model training|\n| architecture | The mathematical function that we pass input data and parameters to |\n|model | The combination of the architecture with a particular set of parameters|  \n|gradient | A Tensor that tells us how the vector field changes in any direction |\n| loss | A measure of how good the model is, chosen to drive training via stochastic gradient descent|\n|CNN| Convolutional Neural Network; a type of NN that works well for computer vision tasks | \n| embeddings | A relatively-low dimension space into which we can translate high-dimensional vectors|\n| latent | A latent is a compressed, rough representation of a higher quality image. It consists of the most important features of an image|\n|noise| Random variation of brightness or color information in images|\n:::\n\nIf you had to make a guess about how image generation works, what would that guess be? Think for a moment. If you're like me, then you probably thought that we try to get the model to spit out cool looking images, right? Well, not exactly. Bear with me for a moment.\n\\ \n\nImagine a magic function that takes in images of handwritten digits as inputs and spits out the probability that the input is a digit. \n\\ \n\nLet the probability be $P(X)$ for an input Image X.\n```{mermaid}\nflowchart LR\n  I1(X1) --> F((Magic Function))\n  I2(X2) --> F\n  I3(X3) --> F\n  F --> O1(\"P(X1)\")\n  F --> O2(\"P(X2)\")\n  F --> O3(\"P(X3)\")\n\n```\n\n\\\n\nWait. How do we use this function to generate new digits?\\\nWe do the following steps:\\\n\n1. Try making a pixel of X darker and then check the output probability\n2. Try making a pixel of X lighter and then check probability\n3. Do this for every pixel in X (If X is a 28x28 image, that's 784 times!)\n\nWhat we're calculating is the probability of X being a handwritten digit wrt the gradient of X. This function is called the `Score Funtion`.\n$$\\nabla_X P(X)$$\n\n`Noisy image = Clear Image + Noise`\\\n\nFull disclaimer that this is an absurdly simplified explanation of it and the real thing is probably much more different and complicated but, it is a good place to build some intuition from.\n\\\n\nProbability of Noise:\n\n\n\n\n\n\\\n\n```{mermaid}\nflowchart LR\n    B[\"Randomly selected sequential noise\"] -->  X((+))\n    X --> A[\"Somewhat noisy latent\"] --> C((\"Diffusion Model\"))\n    C --> D(\"Predicted Noise\") --> - --> A\n    A --> E[\"Less noisy latent\"]\n    F[\"Randomly Selected Noise\"] --> Z((+)) --> E -->\n    G((\"Updated Diffusion Model\")) --> H(\"Predicted Noise\")\n    H --> Y[-] --> E\n```\n\n\\\n\n| Model             | Input                  | Output      | Notes              |\n| ----------------- | ---------------------- | ----------- | ------------------ |\n| Unet              | somewhat noisy latents | the noise   | -                  |\n| VAE Decoder       | small latents tensor   | large image | guided by captions |\n| CLIP text encoder | text                   | embedding   | -                  |\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"slate","title-block-banner":true,"title":"Stable Diffusion for dummies (written by a dummy)","description":"A little informal guide for diffusion first timers.","author":"Harsh Sharma","date":"10/29/2023","image":"logo.png","categories":["Notes","Fastai Part2"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}